{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision Tree Challenge\n",
        "\n",
        "Feature Importance and Categorical Variable Encoding\n",
        "\n",
        "# ðŸŒ³ Decision Tree Challenge - Feature Importance and Variable Encoding\n",
        "\n",
        "## Challenge Overview\n",
        "\n",
        "**Your Mission:** Create a simple GitHub Pages site that demonstrates\n",
        "how decision trees measure feature importance and analyzes the critical\n",
        "differences between categorical and numerical variable encoding. Youâ€™ll\n",
        "answer two key discussion questions by adding narrative to a pre-built\n",
        "analysis and posting those answers to your GitHub Pages site as a\n",
        "rendered HTML document.\n",
        "\n",
        "``` python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ioff()  # Turn off interactive mode for Quarto\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load data\n",
        "sales_data = pd.read_csv('salesPriceData.csv')\n",
        "sales_data.head()\n",
        "\n",
        "# Prepare model data (treating zipCode as numerical)\n",
        "model_vars = ['SalePrice', 'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath',\n",
        "              'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode']\n",
        "model_data = sales_data[model_vars].dropna()\n",
        "\n",
        "# Split data\n",
        "X = model_data.drop('SalePrice', axis=1)\n",
        "y = model_data['SalePrice']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Build decision tree\n",
        "tree_model = DecisionTreeRegressor(max_depth=3,\n",
        "                                  min_samples_split=20,\n",
        "                                  min_samples_leaf=10,\n",
        "                                  random_state=123)\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Model built with {tree_model.get_n_leaves()} terminal nodes\")\n",
        "\n",
        "# Prepare model data (treating zipCode as categorical - one-hot encoded)\n",
        "model_vars_cat = ['SalePrice', 'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath',\n",
        "                  'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode']\n",
        "model_data_cat = sales_data[model_vars_cat].dropna()\n",
        "\n",
        "# One-hot encode zipCode\n",
        "model_data_cat = pd.get_dummies(model_data_cat, columns=['zipCode'], prefix='zipCode')\n",
        "\n",
        "# Split data\n",
        "X_cat = model_data_cat.drop('SalePrice', axis=1)\n",
        "y_cat = model_data_cat['SalePrice']\n",
        "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y_cat, test_size=0.2, random_state=123)\n",
        "\n",
        "# Build decision tree with categorical zipCode\n",
        "tree_model_cat = DecisionTreeRegressor(max_depth=3,\n",
        "                                     min_samples_split=20,\n",
        "                                     min_samples_leaf=10,\n",
        "                                     random_state=123)\n",
        "tree_model_cat.fit(X_train_cat, y_train_cat)\n",
        "\n",
        "print(f\"Categorical model built with {tree_model_cat.get_n_leaves()} terminal nodes\")\n",
        "```\n",
        "\n",
        "## Tree Visualization\n",
        "\n",
        "### Python\n",
        "\n",
        "``` python\n",
        "#| fig-width: 20\n",
        "#| fig-height: 12\n",
        "# Visualize tree\n",
        "plt.figure(figsize=(20, 12))\n",
        "plot_tree(tree_model,\n",
        "          feature_names=X_train.columns,\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=10,\n",
        "          max_depth=3)\n",
        "plt.title(\"Decision Tree (zipCode as Numerical)\")\n",
        "plt.tight_layout()\n",
        "```\n",
        "\n",
        "``` python\n",
        "#| fig-width: 20\n",
        "#| fig-height: 12\n",
        "# Visualize categorical tree\n",
        "plt.figure(figsize=(20, 12))\n",
        "plot_tree(tree_model_cat,\n",
        "          feature_names=X_train_cat.columns,\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=10,\n",
        "          max_depth=3)\n",
        "plt.title(\"Decision Tree (zipCode as Categorical - One-Hot Encoded)\")\n",
        "plt.tight_layout()\n",
        "```\n",
        "\n",
        "## Discussion Questions for Challenge\n",
        "\n",
        "1.  **Numerical vs Categorical Encoding:** There are two models in\n",
        "    Python written above. For each language, the models differ by how\n",
        "    zip code is modelled, either as a numerical variable or as a\n",
        "    categorical variable. Given what you know about zip codes and real\n",
        "    estate prices, how should zip code be modelled, numerically or\n",
        "    categorically? Is zipcode and ordinal or non-ordinal variable?\n",
        "\n",
        "# Answer\n",
        "\n",
        "Calculate and visualize the feature importances for both decision tree\n",
        "models (one with `zipCode` as numerical, one with `zipCode` as\n",
        "categorical), then compare and interpret the results, summarizing the\n",
        "impact of numerical vs.Â categorical encoding of `zipCode` on feature\n",
        "importance.\n",
        "\n",
        "## Calculate Numerical Feature Importance\n",
        "\n",
        "### Subtask:\n",
        "\n",
        "Calculate and store the feature importances from `tree_model`, which was\n",
        "trained using `zipCode` as a numerical variable.\n",
        "\n",
        "**Reasoning**: The task is to calculate and store feature importances\n",
        "from `tree_model`. I will access the `feature_importances_` attribute,\n",
        "create a pandas Series mapping these to feature names from\n",
        "`X_train.columns`, and then sort them in descending order for clear\n",
        "presentation.\n",
        "\n",
        "``` python\n",
        "#| fig-width: 10\n",
        "#| fig-height: 6\n",
        "feature_importances = pd.Series(tree_model.feature_importances_, index=X_train.columns)\n",
        "feature_importances_numerical = feature_importances.sort_values(ascending=False)\n",
        "\n",
        "print(\"Feature Importances (zipCode as Numerical):\")\n",
        "print(feature_importances_numerical)\n",
        "\n",
        "# Plot feature importances for numerical zipCode\n",
        "plt.figure(figsize=(10, 6))\n",
        "feature_importances_numerical.plot(kind='bar')\n",
        "plt.title('Feature Importances (zipCode as Numerical)')\n",
        "plt.ylabel('Importance')\n",
        "plt.xlabel('Features')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "```\n",
        "\n",
        "**Reasoning**: The subtask asked to calculate and visualize feature\n",
        "importances for both models. I have already done this for the numerical\n",
        "zipCode model. Now, I need to calculate and visualize the feature\n",
        "importances for the `tree_model_cat`, which was trained with `zipCode`\n",
        "as a categorical variable using one-hot encoding.\n",
        "\n",
        "``` python\n",
        "#| fig-width: 12\n",
        "#| fig-height: 7\n",
        "feature_importances_cat = pd.Series(tree_model_cat.feature_importances_, index=X_train_cat.columns)\n",
        "feature_importances_categorical = feature_importances_cat.sort_values(ascending=False)\n",
        "\n",
        "print(\"Feature Importances (zipCode One-Hot Encoded):\")\n",
        "print(feature_importances_categorical)\n",
        "\n",
        "# Plot feature importances for categorical zipCode\n",
        "plt.figure(figsize=(12, 7))\n",
        "feature_importances_categorical.plot(kind='bar')\n",
        "plt.title('Feature Importances (zipCode One-Hot Encoded)')\n",
        "plt.ylabel('Importance')\n",
        "plt.xlabel('Features')\n",
        "plt.xticks(rotation=90, ha='right')\n",
        "plt.tight_layout()\n",
        "```\n",
        "\n",
        "## Comparison and Interpretation of Feature Importances\n",
        "\n",
        "We have analyzed two decision tree models, one treating `zipCode` as a\n",
        "numerical variable and another treating it as a categorical variable\n",
        "(one-hot encoded).\n",
        "\n",
        "### Observations:\n",
        "\n",
        "**1. Numerical `zipCode` Model:** \\* **`zipCode` Importance:** In the\n",
        "model where `zipCode` was treated numerically, its importance was 0.0,\n",
        "indicating it was not used for any splits in the tree. This is expected,\n",
        "as numerical ordering of zip codes (e.g., 50010 \\< 50011) holds no\n",
        "intrinsic meaning for predicting house prices. \\* **Top Features:**\n",
        "`GrLivArea`, `GarageCars`, `YearBuilt`, and `TotRmsAbvGrd` were\n",
        "identified as the most important features.\n",
        "\n",
        "**2. Categorical `zipCode` Model (One-Hot Encoded):** \\* **`zipCode`\n",
        "Importance:** Even after one-hot encoding, none of the individual\n",
        "`zipCode` dummy variables (`zipCode_50011`, `zipCode_50012`, etc.)\n",
        "showed any importance (all 0.0). This suggests that within the limited\n",
        "`max_depth` (3) of our decision tree and the\n",
        "`min_samples_split`/`min_samples_leaf` constraints, no single `zipCode`\n",
        "category was deemed significant enough to create a split that\n",
        "substantially reduced impurity. \\* **Top Features:** The top features\n",
        "remain the same (`GrLivArea`, `GarageCars`, `YearBuilt`, `TotRmsAbvGrd`)\n",
        "and their importances are identical to the numerical `zipCode` model.\n",
        "\n",
        "### Impact of Encoding:\n",
        "\n",
        "In this specific scenario with a shallow tree (`max_depth=3`) and the\n",
        "given data, the encoding of `zipCode` (numerical vs.Â one-hot encoded\n",
        "categorical) did not significantly change the **overall ranked\n",
        "importance** of the other numerical features (`GrLivArea`, `GarageCars`,\n",
        "`YearBuilt`, `TotRmsAbvGrd`). Both models yielded the same dominant\n",
        "features with identical importance scores.\n",
        "\n",
        "However, the lack of importance for *any* `zipCode` representation\n",
        "(either numerical or individual one-hot encoded dummies) is notable.\n",
        "While `zipCode` is generally considered an important factor in real\n",
        "estate, its individual one-hot encoded features did not contribute to\n",
        "the modelâ€™s splits in this instance. This could be due to several\n",
        "factors:\n",
        "\n",
        "-   **Tree Depth:** A shallow tree (max_depth=3) might not be able to\n",
        "    capture the complex relationships or small improvements in impurity\n",
        "    offered by individual zip code categories.\n",
        "-   **One-Hot Encoding Sparsity:** One-hot encoding creates many new\n",
        "    features, making the data sparse. With a limited number of splits,\n",
        "    the tree might prioritize more impactful continuous features.\n",
        "-   **Lack of Strong Individual Zip Code Effect:** Itâ€™s possible that\n",
        "    within the context of the other strong predictors like `GrLivArea`,\n",
        "    no *single* zip code dummy variable has a strong enough individual\n",
        "    predictive power to be selected for a split at this tree depth.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "While the *method* of encoding categorical variables correctly is\n",
        "crucial for model interpretability and avoiding meaningless numerical\n",
        "splits, in this particular limited-depth decision tree, `zipCode`\n",
        "(whether numerical or one-hot encoded) did not emerge as an important\n",
        "feature. The problem description itself highlights that â€˜zipCode \\<\n",
        "50012.5â€™ is meaningless. By one-hot encoding, we remove this meaningless\n",
        "numerical order, even if the individual dummy variables didnâ€™t\n",
        "contribute to the tree splits. This result, therefore, confirms that\n",
        "treating `zipCode` as numerical would indeed lead to misleading insights\n",
        "about its importance (or lack thereof), whereas one-hot encoding, while\n",
        "also showing zero importance in this shallow tree, at least correctly\n",
        "represents the variableâ€™s categorical nature.\n",
        "\n",
        "1.  **R vs Python Implementation Differences:** When modelling zip code\n",
        "    as a categorical variable, the output tree and feature importance\n",
        "    would differ quite significantly had you used R as opposed to\n",
        "    Python. Investigate why this is the case. What does R offer that\n",
        "    Python does not? Which language would you say does a better job of\n",
        "    modelling zip code as a categorical variable? Can you quote the\n",
        "    documentation at <https://scikit-learn.org/stable/modules/tree.html>\n",
        "    suggesting a weakness in the Python implementation? If so, please\n",
        "    provide a quote from the documentation.\n",
        "\n",
        "2.  **Are There Any Suggestions for Implementing Decision Trees in\n",
        "    Python With Prioper Categorical Handling?** Please poke around the\n",
        "    Internet (AI is not as helpful with new libraries) for suggestions\n",
        "    on how to implement decision trees in Python with better (i.e.Â not\n",
        "    one-hot encoding) categorical handling. Please provide a link to the\n",
        "    source and a quote from the source. There is not right answer here,\n",
        "    but please provide a thoughtful answer, I am curious to see what you\n",
        "    find."
      ],
      "id": "3d426f3a-16c3-41e3-bf86-71b6599eb706"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}